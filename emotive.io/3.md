# Ex 3

```
Bitly is an important service that basically maps and shortens long URLs and replaces them with a different URL of the form bit.ly/[Some Hashcode]. 
Emotive uses Bitly to match URLs to a bitly link that gets sent out to customers and redirect to the longer link.

The hashcode is usually of the form A-Z and 0-9. Assume Bitly is down for a week and you need to architect a system that functions like bitly. 
Walk through the brief architecture of such a system, talk about the calls, the servers, and the design of the database. 
Be prepared to talk about scaling such a system to higher levels. How would you handle one URL? How would you handle 10 million URLs?
```


## Handling one URL


If we only need to handle a few URLs, we could simply have a web server (say in an AWS EC2 instance or in an AWS S3 bucket) that takes and process the given long URL, returning the short URL:

- For each given long URL, we could create a simple HTML page with URL redirect tag in the head and add that to S3 bucket. The S3 bucket hosting the website would open the page which automatically redirects. 
- Or we could have a web app running in EC2 with the algorithm that would do the encode/decoding. This would compute a unique hash (e.g. MD5) of the given long URL, and the hash then could be encoded to base36 or even base64 (depending on how we want the final short URL to look).


In any of the cases above, since we donâ€™t need to worry about scalability, we would keep our data in a small key-value database.  We could either use:

- a RDBMS, which provides acid properties. We would have two tables: one for storing information about the URL mappings and other for the user info.
- a smaller key-value database such as AWS SimpleDB, or depending on the cases, an in-memory key-value database such as Redis or Memcached. The short URL is the key, the long URL (and any metadata such as date/time), the value.


The API should have the following methods:

- **add new URL**: 
    - get long link URL
    - encode the link URL to a small URL
    - saves both URLs and metadata in a key-value database
        - (optional) save or log request information (for analytics)
        - (optional) add an expiration date for the URLs

- **retrieve long URL, given the short URL**
    - do a database lookup with the short URL:
        - if the URL exists in the database, returns the long URL
            - (optional) log or saves the request information (analytics)
        - if URL does not exist (or expired) return error

- **delete URL entry**
    - do a database lookup with the shorter URL:
        - if the URL exists in the database, delete the entry
        - if URL does not exist (or expired) return error



## Handling 10 million URLs


Now we are concerned about things such as high availability and avoiding URL collisions. Additionally, we might be interested in profit $$, so gathering and processing analytics/metadata/request info etc. is a must.

If we are still using the EC2 or S3 architecture, we might horizontally scale it with a proxy or a pool of hosts (and manager of a pool of hosts). ZooKeeper is a distributed coordination service to manage a large set of hosts and offers key-value store. We could use AWS Elastic Load Balancing as well if we opted for EC2 instances.

We would like to have a more scalable key/value database, such as AWS DynamoDB.

We could have a cache (such as Redis or Memcached) with the most popular URLs, for quick retrieval (and we would take extra care with expiration).

However, in nowadays applications, the architecture lined above is becoming obsolete. Best options have distinct asynchronous and synchronous pipelines.

 For synchronous requests, we can use microservices in containers (and orchestrated by Kubernetes).

For asynchronous requests, depending on the volume of requests, we could either couple microservices with a distributed streaming platforms (such as Kafka), or create a pipeline with a queue (such as AWS SQS) and serverless functions (such as AWS Lambda), with the database.

#### Microservices for URL conversion (synchronous)

The URL conversion should be as faster as possible. We could process the information into backend microservices running into (Docker) containers in a Kubernetes cluster (such as AWS EK). This is ideal for:
- the URL shortener algorithm
- a Spam checker algorithm

Vertical scaling is straightforward as well (in Kubernetes, we could scale by just adding more pods).


#### Distributed streaming platform and Event-driven serverless computing for Event Analytics (asynchronous)

Whenever a link is shortened, we can send a message in a topic in a distributed streaming platform, such as Kafka or AWS Kinesis. Consumers would pick up the data and save into a database, or process the information into backend services running either in event-driven serverless computing platforms (such as AWS Lambda functions) or into microservices. This is ideal for:

- Data analysis, process, machine learning
- Elasticsearch, log process
- extra-availability services, such as writing the short and long URLs into a S3 (say into HDFS format), and then connect a local Memcached instance to cache S3 lookups

